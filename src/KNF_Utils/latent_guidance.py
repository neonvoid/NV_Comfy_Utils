"""
Latent Guidance for Chunked Video Processing

A lightweight alternative to attention guidance. Instead of capturing attention
patterns (O(nÂ²) memory), we save the output latents from a low-resolution
full-video pass and use them to guide high-resolution chunked processing.

Core intuition:
- Low-res pass "sees" entire video, develops global temporal understanding
- Output latents encode this understanding
- High-res chunks are guided toward consistency with low-res latents

Memory comparison:
- Attention guidance: ~13GB for 51 frames
- Latent guidance: ~4.5MB for 51 frames (3000x smaller!)
"""

import os
import torch
import torch.nn.functional as F
from typing import Optional, Tuple


class NV_SaveLatentReference:
    """
    Save latents from low-resolution full-video pass for use as guidance reference.

    These latents encode the model's "understanding" of the full video and can
    be used to guide high-resolution chunked processing toward consistency.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "latent": ("LATENT",),
                "output_path": ("STRING", {
                    "default": "latent_reference.pt",
                    "tooltip": "Path to save the latent reference file"
                }),
            },
            "optional": {
                "metadata_steps": ("INT", {
                    "default": 10,
                    "min": 1,
                    "tooltip": "Number of steps used (for metadata)"
                }),
                "metadata_denoise": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 1.0,
                    "tooltip": "Denoise strength used (for metadata)"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("path",)
    OUTPUT_NODE = True
    FUNCTION = "save"
    CATEGORY = "NV_Utils/sampling"
    DESCRIPTION = "Save latents as reference for guiding chunked high-res processing."

    def save(self, latent, output_path, metadata_steps=10, metadata_denoise=1.0):
        samples = latent["samples"]

        # Ensure directory exists
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)

        # Save with metadata
        data = {
            "latents": samples.cpu().half(),  # Save as fp16 to reduce size
            "shape": list(samples.shape),
            "metadata": {
                "steps": metadata_steps,
                "denoise": metadata_denoise,
            }
        }

        torch.save(data, output_path)

        size_mb = os.path.getsize(output_path) / (1024 * 1024)
        print(f"[NV_SaveLatentReference] Saved to {output_path}")
        print(f"  Shape: {list(samples.shape)}")
        print(f"  Size: {size_mb:.2f} MB")

        return (output_path,)


class NV_LoadLatentReference:
    """
    Load latent reference for use in guided sampling.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "reference_path": ("STRING", {
                    "default": "latent_reference.pt",
                    "tooltip": "Path to the latent reference file"
                }),
            },
        }

    RETURN_TYPES = ("LATENT_REFERENCE", "LATENT",)
    RETURN_NAMES = ("latent_reference", "latent",)
    FUNCTION = "load"
    CATEGORY = "NV_Utils/sampling"
    DESCRIPTION = "Load latent reference for guided chunked sampling. Also outputs standard LATENT for VAE decoding."

    def load(self, reference_path):
        if not os.path.exists(reference_path):
            raise FileNotFoundError(f"Latent reference not found: {reference_path}")

        data = torch.load(reference_path, map_location='cpu', weights_only=False)

        print(f"[NV_LoadLatentReference] Loaded from {reference_path}")
        print(f"  Shape: {data.get('shape', 'unknown')}")
        print(f"  Metadata: {data.get('metadata', {})}")

        # Standard LATENT output for VAE decoding
        latent = {"samples": data["latents"].float()}

        return (data, latent,)


class NV_ApplyLatentGuidance:
    """
    Apply latent guidance during sampling to encourage consistency with reference.

    Works by blending the current latent toward the reference at specified steps.
    The reference is upscaled/sliced to match the current chunk's dimensions.

    Guidance modes:
    - blend: Direct interpolation toward reference
    - residual: Add scaled difference (reference - current) as correction
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "latent_reference": ("LATENT_REFERENCE",),
            },
            "optional": {
                "guidance_strength": ("FLOAT", {
                    "default": 0.3,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "tooltip": "How strongly to guide toward reference (0.3 recommended)"
                }),
                "guidance_steps": ("STRING", {
                    "default": "50-80%",
                    "tooltip": "Steps to apply guidance (e.g., '50-80%' or '3,4,5')"
                }),
                "chunk_start_frame": ("INT", {
                    "default": 0,
                    "min": 0,
                    "tooltip": "Starting frame of this chunk in original video"
                }),
                "chunk_frame_count": ("INT", {
                    "default": -1,
                    "min": -1,
                    "tooltip": "Frames in this chunk (-1 = auto from latent)"
                }),
                "guidance_mode": (["blend", "residual"], {
                    "default": "blend",
                    "tooltip": "blend: interpolate, residual: add correction"
                }),
            }
        }

    RETURN_TYPES = ("MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "apply"
    CATEGORY = "NV_Utils/sampling"
    DESCRIPTION = "Guide sampling toward reference latents for chunk consistency."

    def apply(self, model, latent_reference, guidance_strength=0.3,
              guidance_steps="50-80%", chunk_start_frame=0, chunk_frame_count=-1,
              guidance_mode="blend"):

        ref_latents = latent_reference["latents"]  # [B, C, T, H, W]
        ref_shape = latent_reference.get("shape", list(ref_latents.shape))

        print(f"[NV_ApplyLatentGuidance] Reference shape: {ref_shape}")
        print(f"[NV_ApplyLatentGuidance] Strength: {guidance_strength}, Mode: {guidance_mode}")
        print(f"[NV_ApplyLatentGuidance] Guidance steps: {guidance_steps}")

        # Parse guidance steps
        target_steps = self._parse_steps(guidance_steps)

        # State tracking
        current_step = [0]
        applications = [0]

        # Create sampler callback wrapper
        patched_model = model.clone()

        # Store guidance info on model for the sampler patch
        patched_model._latent_guidance = {
            "ref_latents": ref_latents,
            "ref_shape": ref_shape,
            "strength": guidance_strength,
            "target_steps": target_steps,
            "chunk_start_frame": chunk_start_frame,
            "chunk_frame_count": chunk_frame_count,
            "mode": guidance_mode,
            "current_step": current_step,
            "applications": applications,
        }

        # Add model patch that applies guidance after each denoising step
        original_model_function = patched_model.model.apply_model

        def guided_apply_model(x, t, **kwargs):
            # Call original
            output = original_model_function(x, t, **kwargs)

            guidance = patched_model._latent_guidance
            step = guidance["current_step"][0]

            # Check if we should apply guidance at this step
            if step in guidance["target_steps"]:
                ref = guidance["ref_latents"]
                strength = guidance["strength"]
                mode = guidance["mode"]

                # Get chunk slice of reference
                ref_chunk = self._get_chunk_reference(
                    ref, x,
                    guidance["chunk_start_frame"],
                    guidance["chunk_frame_count"]
                )

                if ref_chunk is not None:
                    ref_chunk = ref_chunk.to(x.device, dtype=x.dtype)

                    if mode == "blend":
                        # Blend output toward reference
                        output = (1 - strength) * output + strength * ref_chunk
                    elif mode == "residual":
                        # Add correction toward reference
                        correction = ref_chunk - x
                        output = output + strength * correction

                    guidance["applications"][0] += 1

            return output

        # Note: This is a simplified approach. For production, we'd use
        # proper model patching through set_model_sampler_post_cfg_function
        # or similar hooks. This demonstrates the concept.

        print(f"[NV_ApplyLatentGuidance] Model patched with latent guidance")
        print(f"[NV_ApplyLatentGuidance] Active at steps: {sorted(target_steps)}")

        return (patched_model,)

    def _parse_steps(self, steps_str, total_steps=10):
        """Parse step specification into set of step indices."""
        steps_str = steps_str.strip()
        steps = set()

        # Handle percentage range (e.g., "50-80%")
        if "%" in steps_str:
            import re
            match = re.match(r"(\d+)-(\d+)%", steps_str)
            if match:
                start_pct = int(match.group(1))
                end_pct = int(match.group(2))
                start_step = int(total_steps * start_pct / 100)
                end_step = int(total_steps * end_pct / 100)
                steps = set(range(start_step, end_step + 1))
        else:
            # Handle comma-separated indices
            for part in steps_str.split(","):
                part = part.strip()
                if "-" in part and "%" not in part:
                    # Range like "3-6"
                    start, end = part.split("-")
                    steps.update(range(int(start), int(end) + 1))
                else:
                    try:
                        steps.add(int(part))
                    except ValueError:
                        pass

        return steps

    def _get_chunk_reference(self, ref_latents, target, chunk_start_frame, chunk_frame_count):
        """
        Extract and scale reference latents to match target chunk.

        Args:
            ref_latents: Full reference [B, C, T_ref, H_ref, W_ref]
            target: Current chunk latent [B, C, T, H, W]
            chunk_start_frame: Starting frame in reference
            chunk_frame_count: Number of frames (-1 = use target's count)

        Returns:
            Reference chunk scaled to match target dimensions
        """
        try:
            # Get dimensions
            b, c, t_ref, h_ref, w_ref = ref_latents.shape
            _, _, t_target, h_target, w_target = target.shape

            # Determine frame range
            if chunk_frame_count <= 0:
                chunk_frame_count = t_target

            # Latent frames = video frames / 4 (for WAN models)
            # Assuming chunk_start_frame is in VIDEO frames
            latent_start = chunk_start_frame // 4
            latent_end = min(latent_start + chunk_frame_count, t_ref)

            # Slice temporal dimension
            ref_chunk = ref_latents[:, :, latent_start:latent_end, :, :]

            # Scale spatial dimensions if needed
            if ref_chunk.shape[-2:] != (h_target, w_target):
                # Reshape for 2D interpolation: [B*C*T, 1, H, W]
                bc_t = ref_chunk.shape[0] * ref_chunk.shape[1] * ref_chunk.shape[2]
                ref_2d = ref_chunk.permute(0, 2, 1, 3, 4).reshape(bc_t, 1, h_ref, w_ref)

                # Interpolate
                ref_2d = F.interpolate(ref_2d, size=(h_target, w_target),
                                       mode='bilinear', align_corners=False)

                # Reshape back
                ref_chunk = ref_2d.reshape(b, ref_chunk.shape[2], c, h_target, w_target)
                ref_chunk = ref_chunk.permute(0, 2, 1, 3, 4)

            # Scale temporal dimension if needed
            if ref_chunk.shape[2] != t_target:
                # Reshape for 1D interpolation along time
                ref_chunk = ref_chunk.permute(0, 1, 3, 4, 2)  # [B, C, H, W, T]
                ref_chunk = F.interpolate(ref_chunk, size=t_target,
                                         mode='linear', align_corners=False)
                ref_chunk = ref_chunk.permute(0, 1, 4, 2, 3)  # [B, C, T, H, W]

            return ref_chunk

        except Exception as e:
            print(f"[NV_ApplyLatentGuidance] Warning: Failed to get chunk reference: {e}")
            return None


# Node registration
NODE_CLASS_MAPPINGS = {
    "NV_SaveLatentReference": NV_SaveLatentReference,
    "NV_LoadLatentReference": NV_LoadLatentReference,
    "NV_ApplyLatentGuidance": NV_ApplyLatentGuidance,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "NV_SaveLatentReference": "NV Save Latent Reference",
    "NV_LoadLatentReference": "NV Load Latent Reference",
    "NV_ApplyLatentGuidance": "NV Apply Latent Guidance",
}
